akka.projection.grpc {
  consumer {
    class = "akka.projection.grpc.consumer.GrpcReadJournalProvider"

    # Note: these settings are only applied when constructing the consumer from config
    #       if creating the GrpcQuerySettings programmatically these settings are ignored

    # Configuration of gRPC client.
    # See https://doc.akka.io/libraries/akka-grpc/current/client/configuration.html#by-configuration
    client = ${akka.grpc.client."*"}
    client {
    }

    # Mandatory field identifying the stream to consume/type of entity, must be a stream id
    # exposed by the producing/publishing side
    stream-id = ""

    # Pass these additional request headers as string values in each request to the producer
    # can be used for example for authorization in combination with an interceptor in the producer.
    # Example "x-auth-header": "secret"
    additional-request-headers {}

    filter {
      ddata-read-timeout = 3s
      ddata-write-timeout = 3s
    }

    push-destination {
      # Parallelism per connected push producer
      parallelism = 100
      # Disconnect a producer flow if an event is not written to database within this timeout
      # producer will reconnect, possibly after backoff.
      journal-write-timeout = 5s
    }
  }

  producer {
    # Query plugin for eventsBySlices, such as "akka.persistence.r2dbc.query".
    query-plugin-id = ""

    # When using async transformations it can be good to increase this.
    transformation-parallelism = 1

    filter {
      replay-parallelism = 3
      # Topic of an event is defined by an event tag with this prefix
      topic-tag-prefix = "t:"
    }

    # Interval to send keep alives to keep firewalls etc from closing connections with
    # large pauses between events going through. Set to 0 to disable keep alive messages.
    #
    # Note: Currently only used by the event producer push
    keep-alive-interval = 5s

    # See event-origin-filter-enabled
    max-ack-cache-entries = 1000

  }

  replication {

    # Allow edge replicas to connect and replicate updates
    accept-edge-replication = off

    # Replicated event sourcing from edge sends each event over sharding, in case that delivery
    # fails or times out, retry this number of times, with an increasing backoff conntrolled by
    # the min and max backoff settings
    edge-replication-delivery-retries = 3
    edge-replication-delivery-min-backoff = 250ms
    edge-replication-delivery-max-backoff = 5s

    # By default, the origin of the event is taken into account in a way that the consumer will
    # only receive the event directly from the origin replica if it's configured to be connected
    # to it. If the consumer isn't directly connected to the origin replica, it will receive those
    # events indirectly via other replicas that it is consuming from. This is an efficient way to
    # avoid sending duplicates from all replicas.
    #
    # However, one consequence of this is that event causality isn't preserved when using more than
    # 2 replicas. This may not matter when using CRDTs that can compensate for the lack of ordering.
    # Set this property to off if you want to preserve causal ordering. There is still an best effort
    # acknowledgement and deduplication mechanism to avoid transfer of full event payloads from all
    # replicas.
    event-origin-filter-enabled = on
  }

}

akka {
  actor {
    serializers {
      akka-projection-grpc-consumer = "akka.projection.grpc.internal.ConsumerSerializer"
    }
    serialization-identifiers {
      "akka.projection.grpc.internal.ConsumerSerializer" = 1558148901
    }
    serialization-bindings {
      "akka.projection.grpc.internal.DdataConsumerFilterStore$State" = akka-projection-grpc-consumer
      "akka.projection.grpc.internal.DdataConsumerFilterStore$ConsumerFilterKey" = akka-projection-grpc-consumer
      "akka.projection.grpc.internal.ConsumerFilterRegistry$PubSubAck" = akka-projection-grpc-consumer
    }
  }
}
